---
title: Linear models lecture
author: Jonathan Dushoff and Ben Bolker
bibliography: ../qmee.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))
```

# Introduction
 
## History
 
- ANOVA, ANCOVA, regression, $t$ test are **all (part of) the same thing**, the **general** linear model ([twitter thread](https://twitter.com/MaartenvSmeden/status/1365036506751135746))
- "linear model" (`lm` in R), as opposed to to distinguish it from the *generalized* linear model (`glm` in R)
- Unfortunately SAS calls it `PROC GLM`

## (part of) the statistical universe

![](../pix/models-glmm.png)

## Extensions [@faraway_extending_2016]
 
- **Generalized** linear models
    - (Some) non-linear relationships
    - Non-normal response *families* (binomial, Poisson, ...)
 - **Mixed** models incorporate *random effects*
    - Categories in data represent samples from a population
    - e.g. species, sites, genes ... experimental blocks
 - Generalized **additive** models (GAMs)
    - much broader range of non-linear relationships
 
# Basic theory

## Assumptions
 
- *Response variables* are linear functions of *predictor (derived) variables*, in turn based on *input (observed) variables*
     - One or more predictor variables per input variable
     - Each predictor variable is associated with an estimated parameter (more about this later)
- Independence
- *Errors* or *residuals* are Normally distributed
     - In other words, the difference between our model *predictions*
       and our observations is Normal
     - *not* assuming the marginal distribution is Normal
- (Predictor variables measured without error)
 
## Machinery
 
- Leads naturally to a *least squares* fit - minimize the squared differences between predictions and observations
- LS fits have a lot of nice properties, including partitioning, and finding the middle
- Solution is a simple (!?) matrix equation
- Sensitive to some violations of assumptions - anomalous events have a larger effect than they should
 
## One-parameter variables
 
- Continuous predictor: straight line, one parameter 
     - Also implies one *input variable*
- $Y = a+bX$: $Y$ is the response, $X$ is the input variable   
($b$ is the *slope* - expected change in $Y$ per unit change in $X$)
- Categorical predictor with two categories: one parameter
     - difference in predicted value between levels, or
    	code the categories as 0, 1 ... (*dummy variables*)
- Parameters are (usually) easy to interpret
- Think in terms of *confidence intervals* for the parameter
 
## Multi-parameter variables
 
- With more than two categories, there is more than one
input variable (parameter) associated with a single predictor variable
    - Why can't we just code them, for example as 0, 1, 2?
- Non-linear response to a predictor variable
    - Might be able to use a linear model! $Y = a + bX + cX^2$ is linear
in $a$ and $b$ (the unknowns)
    - Polynomials can be problematic: use either *orthogonal polynomials* (`poly` in R, e.g. `y  ~ poly(x,2)`) or splines (`splines::ns()`, or `mgcv` package)

## Multi-parameter variables are hard

- If you're just trying to control for it, include it in the model and then ignore it!
- Can (and should) get a $p$ value for the variable as a whole
- But we can only get CIs on the parameters --- and there are different ways to parameterize (*contrasts*)
     - If you do have (a) clear scientific question(s), you can set up contrasts that allow you to test it/them (we can help)
     - If you must, do pairwise comparisons, test each pair of variables for differences and make an `aabbc` list (more later)

## Interactions
 
- Interactions allow the value of one predictor to affect the
relationship between another predictor and the response variable
- Interpreting *main effects* in the presence
of interactions is tricky (*principle of marginality*)
- Your estimate of the effect of variable $B$ now varies
- You need to pick a fixed point, or average in some way
- Example: $Y = a + b_1 X_1 + b_2 X_2 + b_{12} X_1 X_2$
- The response to $X_1$ is $Y = (a+b_2 X_2) + (b_1+b_{12}X_2) X_1$  
the response to $X_1$ *depends on* the value of $X_2$.
 
## An experimental example
 
- You want to know whether a drug treatment changes the
	metabolism of some rabbits
- You're using adult, prime-aged rabbits and keeping them under
	controlled conditions, so you don't expect their metabolism to
	change *without* the drug.
    - Not good enough!
- You also introduce some control rabbits and treat them exactly the
same, including giving them fake pills. You find no significant
change in the metabolism of the control rabbits through time
    - Still not good enough! Why?
 
## Testing interactions
 
- Use an *interaction*:
$$
M = a + B_x X + B_t t + B_{xt} Xt
$$
- The interaction term $B_{xt}$ represents the *difference in the response* between the two groups.
- It asks: **did the treatment group change differently than the control group**?
 
## Interactions and parameters
 
- In simple cases, the interaction may use only
one parameter
- We can use CIs, and coefficient plots, and get a pretty good idea what's
going on
- In complicated cases, interaction terms have many parameters 
- These have all the interpretation problems of other multi-parameter
variables, or more
- Think about "differences in differences"

## Interactions: example

- Bear road-crossing
- Predictor variables: sex (categorical), road type (categorical: major/minor), road length (continuous)
- Two-way interactions
     - sex $\times$ road length: "are females more sensitive to amount of road than males?"
	 - sex $\times$ road type: "do females vary behaviour between road type more than males?"
	 - road type $\times$ road length: "does amount of road affect crossings differently for different road types?"

## Statistical philosophy
 
- Don't accept the null hypothesis
    - Don't throw out predictors you wanted to test because
		they're not significant
	- Don't throw out interactions you wanted to test because
		they're not significant
- This may make your life harder, but it's worth it for the karma
     - 	There are techniques to deal with multiple predictors (e.g.,
ridge or lasso regression)
     - 	There are ways to estimate sensible main effects in the presence of
interactions (centering variables [@schielzeth_simple_2010]; "sum-to-zero contrasts")
 
## Diagnostics
 
- Because the linear model is sensitive (sometimes!) to
assumptions, it is good to evaluate them
- Concerns (in order):
    - *Bias*/*linearity*: did you miss a nonlinear relationship?
    - *Heteroscedasticity* (does variance change across the
		data set, e.g. increasing variance with increasing mean?)
	- Normality (assuming no overall problems, do your
		**residuals** look Normal?)
    - Independence is hard to test
- Normality is the **least important** of these assumptions

## Default plots in R

```{r diagplots}
skewdata <- read.csv("../data/skewdat.csv")
m1 <- lm(skew~Size, data=skewdata)
par(mfrow=c(2,2),mar=c(2,3,1.5,1),mgp=c(2,1,0))
plot(m1,id.n=4)
```

You can also use `broom::augment()` to get all the pieces (fitted, residuals, etc.) that you need to construct these plots for yourself.

---

```{r plotstuff}
library(dplyr)
library(ggplot2); theme_set(theme_bw())
aa <- broom::augment(m1) %>% mutate(n=1:n())
(ggplot(skewdata, aes(Size,skew))
    + geom_point(aes(colour=abs(aa$.resid)>0.1))
    + geom_smooth(method="lm")
    + scale_colour_manual(values=c("black","red"))
)
```
 
## Transformations
 
- One way to deal with problems in model assumptions is by
	transforming one or more of your variables
- Transformations are not cheating: a transformed scale may be as
natural (or more natural) a way to think about your data as your
original scale
- The linear scale (no transformation) often has direct meaning, if
you are adding things up or scaling them (as in our ant example)
- The log scale is often the best scale for thinking about physical
quantities: 1:10 as 10:?
- The *log odds*, or *logit*, scale is often the best scale for
thinking about probabilities: 1%:10% as 10%:?
 
## Transformation tradeoffs
 
- A transformation may help you meet model assumptions
    - Homoscedasticity
    - Linearity
    - Normality
- But there is no guarantee that you can fix them all
- Piles of zeros are hard too (consider GLMs)
 
## Transformations to consider
 
- log-lin, lin-log and log-log for various sorts of exponential and
power relationships
- Box-Cox and [related transformations](https://www.rdocumentation.org/packages/car/versions/3.0-10/topics/bcPower)
- Avoid classical 'transform then linear model' recommendations for
     - probability data (logistic, arcsin or arcsin-square root) [@warton_arcsine_2011]
		or count data (log, log(1+x) or square root) [@ohara_not_2010; @ives_for_2015; @warton_three_2016; @morrissey_revisiting_2020]
	 - Generally better to respect the structure of these data with a GLM
 
## Deciding whether to transform
 
- It's **not OK** to pick transformations based on trying different
ones and looking at $p$ values
- *Box-Cox transformation* tries out transformations of the form
$(y^\lambda-1)/\lambda$ ($\lambda=0$ corresponds to log-transformation) (*positive* data only)
 
# Tools for fitting and inference

## Basic tools

- `lm` fits a linear model
- `summary` prints statistics associated with the *parameters* that were fitted
- `dotwhisker::dwplot()` plots a **coefficient table** [@gelman_lets_2002]
- `car::Anova()` and `drop1()` will find $p$ values that test the effect of
variables that have more than one parameter. `anova(model1, model2)` tests the difference between two specific models (less useful)

## Plotting

- `plot` applied to an `lm` object gives you **diagnostic** plots
- In `ggplot`, `geom_smooth(method="lm")` fits a linear model
	to each group of data (i.e.  each group that you have identified by
	plotting it in a different colour, within a different facet, etc.
- `effects`, `emmeans`, `sjPlot` packages ...
- tools:
    - `predict` gives predicted values, and standard errors.
    - `simulate` simulates values from the fitted model
    - `methods(class="lm")` shows lots of possibilities

# Examples

## Simple model

[data](../data/lizards.csv) 

```{r tests1}
lizards <- read.csv("../data/lizards.csv", stringsAsFactors=TRUE)
lizards$time <- factor(lizards$time,levels=c("early","midday","late"))
lmint <- lm(grahami~light*time, data=lizards)
lmboth <- lm(grahami~light + time, data=lizards)
lmlight <- lm(grahami~light, data=lizards)
lmtime <- lm(grahami~time, data=lizards)
```


## Summary

```{r sum_lizards}
summary(lmboth)
```

## Test time by comparing with light model (dropping time)

```{r tests1B}
anova(lmboth, lmlight, test="F")
```

## Test light (identical to test in summary()

```{r test_light}
anova(lmboth, lmtime, test="F")
## In this simple case, drop1 and car::Anova
##  will do exactly the same thing we did above
```

## Test both

```{r drop1}
drop1(lmboth, test="F")
```

## Better anova table

```{r car_anova}
car::Anova(lmboth)
```

## Interaction model

```{r test2}
print(summary(lmint))
```

# Multiple comparisons

## Multiple comparisons

- Multiple comparisons are tricky
- can get away without them for a linear model (use variable-level $p$ values) *unless you want to do pairwise comparisons*. 
- Unlike variable-level tests, MCs depend on how you set up your model (i.e. centering of continuous variables, contrasts of categorical variables)

## MC example

```{r multcomp,message=FALSE}
library(multcomp)
print(mc <- glht(lmboth, linfct=mcp(time="Tukey")))
```

## compact letter displays

There is a tradition of figuring out which *pairs* of treatments are significantly different and assigning a letter to each such "not-clearly-not-equivalent" group. If you need to do this to make your supervisor happy, you can [use ggpubr](http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/76-add-p-values-and-significance-levels-to-ggplots/). `multcomp` can do the basic computations too.

```{r mc_cld}
cld(mc)
```

## Pairwise comparisons with emmeans

See [emmeans vignette on comparisons](https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html)

```{r emmeans,message=FALSE}
library(emmeans)
e1 <- emmeans(lmboth, "time")
pairs(e1)
```

## Just look at the picture?

```{r empix,fig.height=3}
print(pairs(e1))
## References
