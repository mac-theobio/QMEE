---
title: "Generalized linear models"
author: Ben Bolker and Jonathan Dushoff
date: "`r format(Sys.time(), '%d %B %Y')`"
bibliography: ../qmee.bib
---

```{r pkgs,message=FALSE,echo=FALSE}
library(ggplot2); theme_set(theme_bw())
knitr::opts_chunk$set(echo=FALSE,
                      dev.args = list(png = list(type = "cairo")))
```

# Basics

## Basics

- in R: `glm()`, model specification as before:
`glm(y~f1+x1+f2+x2, data=..., family=..., ...)`
- definition: *family*/*link function*

## Family

- family: what kind of data do I have?
    - from **first principles**: family specifies the relationship between the mean and variance
	- `family=binomial`: proportions, out of a total number of counts; includes binary (Bernoulli) ("logistic regression")
	- `family=poisson`: Poisson (independent counts, no set maximum, or far from the maximum)
	- other (Normal (`"gaussian"`), Gamma)
- default family for `glm` is Gaussian

## Most GLMs are logistic regression

```{r echo=FALSE}
sscrape <- function(string="logistic+regression") {
    require("stringr")
    sstring0 <- "http://scholar.google.ca/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=STRING&as_oq=&as_eq=&as_occt=any&as_sauthors=&as_publication=&as_ylo=&as_yhi=&as_sdt=1.&as_sdtp=on&as_sdts=5&hl=en"
    sstring <- sub("STRING",string,sstring0)
    rr <- suppressWarnings(readLines(url(sstring)))
    ## rr2 <- rr[grep("[Rr]esults",rr)[1]]
    rr2 <- rr
    rstr <- gsub(",","",
                 gsub("</b>.+$","",
                      gsub("^.+[Rr]esults.+of about <b>","",rr2)))
    rstr <- na.omit(str_extract(rr2,"About [0-9,]+ results"))
    rnum <- as.numeric(gsub(",","",str_extract(rstr,"[0-9,]+")))
    attr(rnum,"scrape_time") <- Sys.time()
    return(rnum)
}
``` 

```{r gscrapedata}
fn <- "gscrape.RData"
## could use a caching solution for Sweave (cacheSweave, weaver package,
##  pgfSweave ... but they're all slightly wonky with keep.source at
##  the moment
if (!file.exists(fn)) {
  gscrape <- sapply(c("generalized+linear+model",
                      "logistic+regression","Poisson+regression","binomial+regression"),sscrape)
  save("gscrape",file=fn)
} else load(fn)
```       

```{r gscrapepix,message=FALSE,fig.width=6,fig.height=3}
d <- data.frame(n=names(gscrape),v=gscrape)
d$n <- reorder(d$n,d$v)
print(ggplot(d,aes(x=v,y=n))+geom_point(size=5)
      + scale_x_log10(expand=expansion(0.1,0))
      + geom_text(aes(label=v),colour="red",vjust=2)
      + labs(y="",x="Google Scholar hits")
      )
```

## Link functions

- on what scale are the relationship(s) between predictor(s) and response linear?
- link function goes from *data* scale (counts, proportions etc.) to *effect* or *link* or *linear predictor* scale
    - Poisson=log; binomial=logit
- *inverse link* function goes from effect scale to data scale
    - Poisson=exponential; binomial=logistic
- each family has a default "canonical" link (sensible + nice math)
    - default usually OK (*except*: use `family=Gamma(link="log")`)

## Machinery

- "linear predictor" $\eta = \beta_0 + \beta_1 x_1 + \ldots$ describes patterns on the link scale
- Fit **doesn't transform the responses**: instead applies *inverse* link function to the linear predictor 
    - instead of $\log(y) \sim x$, we analyze $y \sim \mathrm{Poisson}(\exp(x))$
- this is good, because the observed value of $y$ might be zero
    - e.g. count (Poisson) phenotype vs. temperature (centered at 20 C)
	- with $\beta=\{1,1\}$, $T=15$, $\textrm{counts} \sim \textrm{Poisson}(\lambda=\exp(-4)=`r round(exp(-4),3)`)$

## Machinery (2)

Model setup is the same as linear models

- categorical vs. continuous predictors
- contrasts
- interactions
- multivariable regression vs ANOVA vs ANCOVA vs ...

but the linear relationship is set up on the link scale

## log/exponential function (`log`/`exp`): count data

``` {r logpic, echo=FALSE,fig.width=8,fig.height=4}
par(las=1,bty="l")
op <- par(mfrow=c(1,2),oma=c(0,3,0,0),xpd=NA,mgp=c(2,1,0))
curve(exp(x),from=-4,to=4,xlab="x (log-counts)",ylab="counts", main="inverse-link")
curve(log(x),from=exp(-4),to=exp(4),xlab="x (counts)",ylab="log(x)",
      main="link")
par(op)
```


## logit/logistic function (`qlogis`/`plogis`)

``` {r logitpic, echo=FALSE,fig.width=8,fig.height=4}
par(las=1,bty="l")
op <- par(mfrow=c(1,2),oma=c(0,3,0,0),xpd=NA)
curve(plogis(x),from=-4,to=4,xlab="x (log-odds)",ylab="logistic(x)\n(probability)", main="inverse-link")
curve(qlogis(x),from=plogis(-4),to=plogis(4),xlab="x (probability)",ylab="logit(x)", main="link")
par(op)
```

## diagnostics

- harder than linear models: `plot` is still somewhat useful
- binary data especially hard
- goodness of fit tests, $R^2$ etc. hard (can always compute `cor(observed,predict(model, type="response"))`)
- residuals are *Pearson residuals* by default [`(obs-exp)/sqrt(V(exp))`]
- predicted values on the effect scale by default: use `type="response"` to back-transform
- `performance::check_model()`, `DHARMa` package are OK (`simulateResiduals(model,plot=TRUE)`)

## overdispersion

- too much variance: (residual deviance)/(residual df) should be $\approx 1$.  (Ratio >1.2 worrisome; ratio>3, v. worrisome (check your model & data!)
- **quasi-likelihood models** (e.g. `family=quasipoisson`); fit, then adjust CIs/p-values
- alternatives:
    - Poisson $\to$ negative binomial (`MASS::glm.nb`)
	- binomial $\to$ beta-binomial (`glmmTMB` package)
- overdispersion **not relevant** for
    - binary responses
	- families with estimated scale parameters (Gaussian, Gamma, NB, ...)
	- models that already account for it (NB, quasi-likelihood)

## parameter interpretation

- as with linear models (change in response per change in input)
- but on *link* scale
- log link: proportional for small $\beta$, changes
     - e.g. $\beta=0.01 \to$ "$\approx$ 1% change per unit change in input"
	 - $\beta=3 \to$ "$(e^3) \approx$ `r round(exp(3))`-fold change per change in input"

## logit link

- log odds
- odds = $p/(1-p)$ (1% probability is 1:99; 50% = 1:1)
- effect of 1 unit change = **multiply** by **odds ratio** (neutral=1)
- ... or add **log-odds** = logit (neutral=0)

## interpreting logit scale

- effect on the prob scale **depends on baseline probability**
    - low baseline prob: like log link
    - high baseline prob: prop. change in (1-prob)
	- medium prob: absolute change $\approx \beta/4$
	- e.g. going from log-odds of 0 to 1; estimated $\Delta$ prob $\approx$ 0.25
      - `plogis(0)=` `r round(plogis(0),2)`
      - `plogis(0+1)=` `r round(plogis(1),2)`
 - also see [UCLA FAQ on odds ratios](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/); read Gelman and Hill's *Applied Regression Modeling* book (p. 81; [Google books link](https://books.google.ca/books?id=lV3DIdV0F9AC&printsec=frontcover&dq=gelman+hill+logistic+parameter&hl=en&sa=X&ved=0ahUKEwjt8Zq_1-bgAhUqzIMKHTl0D3EQ6AEIKjAA#v=snippet&q=divide%20by%204&f=false))
			  
## inference

- Wald $Z$ tests (i.e., results of `summary()`), confidence intervals
	- approximate, can be way off if parameters have extreme values (*complete separation*)
	- asymptotic (finite-size correction/"degrees of freedom" are hard, usually ignored)
- likelihood ratio tests (equivalent to  $F$ tests); `drop1(model,test="Chisq")`, `anova(model1,model2)`), profile confidence intervals (`MASS::confint.glm`)
- AIC etc.

## Model procedures

- formula like `lm`
- specify family (variance-mean), link (nonlinearity)
- always do Poisson, binomial regression on *counts*, never proportions (although can specify response as a proportion if you also give $N$ as the `weights` argument)
    - Use *offsets* to address unequal sampling
- **always check for overdispersion** if necessary (Poisson/binomial $N>1$)
- if you want to quote values on the original scale, confidence intervals need to be back-transformed; *never back-transform standard errors alone*

## binomial models

- for Poisson, Bernoulli (0/1) responses we only need one piece of information
- how do we specify denominator ($N$ in $k/N$)?
- traditional R: response is two-column matrix `cbind(successes,failures)` [**not** `cbind(successes,total)`]
- also allowed: response is proportion ($k/N$), also specify `weights=N`  
(if equal for all cases and specified on the fly need to replicate:  
`glm(p~...,data,weights=rep(N,nrow(data)))`)

## offsets

- constant terms added to a model
- what if we want to model densities rather than counts?
- log-link (Poisson/NB) models: $\mu_0 = \exp(\beta_0 + \beta_1 x_1 + ...)$
- if we know the area then we want $\mu = A \cdot \mu_0$
- equivalent to adding $\log(A)$ to the linear predictor ($\exp(\log(\mu_0) + \log(A)) = \mu_0 \cdot A$)
- use `... + offset(log(A))` in R formula
- for survival/event modeling over different periods of time, a similar offset trick works with `link="cloglog"` (see [here](https://stats.stackexchange.com/questions/148699/modelling-a-binary-outcome-when-census-interval-varies))

## add-on packages

- `ggplot2`
    - `geom_smooth(method="glm", method.args=list(family=...))`
- `dotwhisker`, `emmeans`, `effects`, `sjPlot`
    - need to interpret parameters appropriately
    - means may be computed on link *or* response scale

## Advanced topics

- complete separation
- ordinal data
- zero-inflation
- non-standard link functions
- visualization (hard because of overlaps: try `stat_sum`, `position="jitter"`, `geom_dotplot`,
([beeswarm plot](http://stackoverflow.com/questions/11889353/avoiding-overlap-when-jittering-points beeswarm plot]))
- see also: GLM extensions talk ([source](https://github.com/bbolker/iiscvisit/blob/master/workshops/glm_extensions.rmd))

## Common(est?) `glm()` problems

- neglecting overdispersion
- binomial/Poisson models with non-integer data
- equating negative binomial with binomial rather than Poisson
- failing to specify `family` ($\to$ linear model);
using `glm()` for linear models (unnecessary)
- predictions on effect scale
- using $(k,N)$ rather than $(k,N-k)$ in binomial models
- worrying about overdispersion unnecessarily (binary/Gamma)
- back-transforming SEs rather than CIs
- Poisson for *underdispersed* responses
- ignoring random effects

# Example

## AIDS in Australia [@dobson_introduction_2008]

```{r nowecho,echo=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

data [here](../data/aids.csv)

```{r aids_ex_1.R,fig.width=10,fig.height=5}
aids <- read.csv("../data/aids.csv")
aids <- transform(aids, date=year+(quarter-1)/4)
gg0 <- ggplot(aids,aes(date,cases))+geom_point()
```

## Easy GLMs with ggplot

```{r ggplot1}
gg1 <- gg0 + geom_smooth(method="glm",colour="red",
                         formula=y~x,
                         method.args=list(family="quasipoisson"))
```

## results

```{r print-ggplot1, echo = FALSE}
print(gg1)
```

## Equivalent code

```{r aids_model_1.R,results="hide"}
g1 <- glm(cases~date, data = aids, family=quasipoisson(link="log"))
summary(g1)
```

## Diagnostics (`plot(g1)`)

```{r diagplot,echo=FALSE,fig.width=6,fig.height=6}
op <- par(mfrow=c(2,2), mar=c(2.5,2.1,1.4,1.1), mgp=c(1.5, 1, 0)) ## set 2x2 grid of plots
plot(g1) ## ugh
par(op)  ## restore parameter settings
```

## autocorrelation function

```{r acf1}
acf(residuals(g1)) ## check autocorrelation
```

## DHARMa

```{r dharma, message=FALSE}
library(DHARMa)
g0 <- update(g1, family=poisson)
plot(simulateResiduals(g0))
```

## ggplot: try quadratic model

```{r ggplot2,fig.width=6,fig.height=4}
print(gg2 <- gg1+geom_smooth(method="glm",formula=y~poly(x,2),
                             method.args=list(family="quasipoisson")))
```

(see [here](https://stackoverflow.com/a/30000214/190277), [here](https://stackoverflow.com/a/31473582/190277) for information on `poly()`)

## improved model

``` {r aids_model_2.R}
g2 <- update(g1,.~poly(date,2))
```

## new diagnostics

```{r aids_test,echo=FALSE}
op <- par(mfrow=c(2,2), mar=c(2,2,1,1), mgp=c(1.5, 1, 0))
plot(g2) ## better
par(op)  ## restore parameter settings
```

## autocorrelation function

```{r acf2}
acf(residuals(g2)) ## check autocorrelation
```

## inference

```{r inf2}
summary(g2)
anova(g1,g2,test="F") ## for quasi-models specifically
```

## References
