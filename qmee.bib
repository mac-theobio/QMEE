
@article{paczolt_multiple_2015,
	title = {Multiple Mating and Reproductive Skew in Parental and Introgressed Females of the Live-Bearing Fish {{\em Xiphophorus birchmanni}}},
	volume = {106},
	issn = {0022-1503},
	url = {https://academic.oup.com/jhered/article/106/1/57/2960035},
	doi = {10.1093/jhered/esu066},
	language = {en},
	number = {1},
	urldate = {2019-02-03},
	journal = {Journal of Heredity},
	author = {Paczolt, Kimberly A. and Passow, Courtney N. and Delclos, Pablo J. and Kindsvater, Holly K. and Jones, Adam G. and Rosenthal, Gil G.},
	month = jan,
	year = {2015},
	pages = {57--66}
}

@article{hurlbert_pseudoreplication_1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {http://www.esajournals.org/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre—layout (or conventional) and layout—specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.  See full-text article at JSTOR},
	number = {2},
	urldate = {2015-11-08},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = jun,
	year = {1984},
	pages = {187--211}
}

@article{davies_dont_2015,
	title = {Don't let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring)},
	copyright = {© 2015 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {2045-7758},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/ece3.1782/abstract},
	doi = {10.1002/ece3.1782},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects where treatments are not replicated and/or replicates are not statistically independent. It is a genuine but controversial issue in ecology particularly in the case of costly landscape-scale manipulations, behavioral studies where ethics or other concerns may limit sample sizes, ad hoc monitoring data, and the analysis of natural experiments where chance events occur at a single site. Here key publications on the topic are reviewed to illustrate the debate that exists about the conceptual validity of pseudoreplication. A survey of ecologists and case studies of experimental design and publication issues are used to explore the extent of the problem, ecologists’ solutions, reviewers’ attitudes, and the fate of submitted manuscripts. Scientists working across a range of ecological disciplines regularly come across the problem of pseudoreplication and build solutions into their designs and analyses. These include carefully defining hypotheses and the population of interest, acknowledging the limits of statistical inference and using statistical approaches including nesting and random effects. Many ecologists face considerable challenges getting their work published if accusations of pseudoreplication are made – even if the problem has been dealt with. Many reviewers reject papers for pseudoreplication, and this occurs more often if they haven't experienced the issue themselves. The concept of pseudoreplication is being applied too dogmatically and often leads to rejection during review. There is insufficient consideration of the associated philosophical issues and potential statistical solutions. By stopping the publication of ecological studies, reviewers are slowing the pace of ecological research and limiting the scope of management case studies, natural events studies, and valuable data available to form evidence-based solutions. Recommendations for fair and consistent treatment of pseudoreplication during writing and review are given for authors, reviewers, and editors.},
	language = {en},
	urldate = {2015-11-08},
	journal = {Ecology and Evolution},
	author = {Davies, G. Matt and Gray, Alan},
	month = oct,
	year = {2015},
	keywords = {Bayesian statistics, confounded effects, hypothesis formation, nesting, peer review, P-values, random effects, scientific publication, statistical population}
}

@article{gelman_beyond_2014,
	title = {Beyond Power Calculations: Assessing Type {S} (Sign) and Type {M} (Magnitude) Errors},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/content/9/6/641},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2015-11-08},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	pmid = {26186114},
	keywords = {design calculation, exaggeration ratio, power analysis, replication crisis, statistical significance, Type M error, Type S error},
	pages = {641--651}
}

@misc{lenth_java_2006,
	title = {Java {Applets} for {Power} and {Sample} {Size} [computer software]},
	url = {http://www.stat.uiowa.edu/~rlenth/Power},
	author = {Lenth, R. V.},
	year = {2006}
}


@article{faul_statistical_2009,
	title = {Statistical power analyses using {G}*{Power} 3.1: {Tests} for correlation and regression analyses},
	volume = {41},
	issn = {1554-3528},
	shorttitle = {Statistical power analyses using {G}*{Power} 3.1},
	url = {https://doi.org/10.3758/BRM.41.4.1149},
	doi = {10.3758/BRM.41.4.1149},
	abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
	language = {en},
	number = {4},
	urldate = {2019-03-17},
	journal = {Behavior Research Methods},
	author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
	month = nov,
	year = {2009},
	keywords = {Effect Size Measure, Implicit Association Test, Linear Multiple Regression, Multiple Correlation Coefficient, Noncentrality Parameter},
	pages = {1149--1160}
}


@article{gerard_limits_1998,
	title = {Limits of {Retrospective} {Power} {Analysis}},
	volume = {62},
	issn = {0022541X},
	url = {http://www.jstor.org/stable/3802357},
	abstract = {Power analysis after study completion has been suggested to interpret study results. We present 3 methods of estimating power and discuss their limitations. We use simulation studies to show that estimated power can be biased, extremely variable, and severely bounded. We endorse the practice of computing power to detect a biologically meaningful difference as a tool for study planning but suggest that calculation of confidence intervals on the parameter of interest is the appropriate way to gauge the strength and biological meaning of study results.},
	number = {2},
	urldate = {2010-10-25},
	journal = {The Journal of Wildlife Management},
	author = {Gerard, Patrick D. and Smith, David R. and Weerakkody, Govinda},
	month = apr,
	year = {1998},
	note = {ArticleType: research-article / Full publication date: Apr., 1998 / Copyright © 1998 Allen Press},
	pages = {801--807}
}

@article{thomas_retrospective_1997,
	title = {Retrospective {Power} {Analysis}},
	volume = {11},
	issn = {0888-8892},
	url = {http://onlinelibrary.wiley.com/doi/10.1046/j.1523-1739.1997.96102.x/abstract;jsessionid=6D27EBA7A0ED9B0498B38A0F03A0D521.d02t01},
	doi = {10.1046/j.1523-1739.1997.96102.x},
	number = {1},
	urldate = {2010-10-25},
	journal = {Conservation Biology},
	author = {Thomas, Len},
	month = feb,
	year = {1997},
	pages = {276--280}
}


@book{bolker_ecological_2008,
	title = {Ecological {Models} and {Data} in {R}},
	isbn = {0-691-12522-8},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	month = jul,
	year = {2008}
}


@article{hoenig_abuse_2001,
	title = {The {Abuse} of {Power}},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001300339897},
	doi = {10.1198/000313001300339897},
	abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
	number = {1},
	urldate = {2019-03-18},
	journal = {The American Statistician},
	author = {Hoenig, John M. and Heisey, Dennis M.},
	month = feb,
	year = {2001},
	keywords = {Bioequivalence testing, Burden of proof, Observed power, Retrospective power analysis, Statistical power, Type II error},
	pages = {19--24}
}

@article{baath_state_2012,
	title = {The {State} of {Naming} {Conventions} in {R}},
	volume = {4},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2012/RJ-2012-018/index.html},
	language = {en},
	number = {2},
	urldate = {2021-01-14},
	journal = {The R Journal},
	author = {Bååth, Rasmus},
	year = {2012},
	pages = {74--75}
}


@misc{bryan_project-oriented_2017,
	title = {Project-oriented workflow},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
	language = {en-us},
	urldate = {2021-01-14},
	journal = {Tidyverse},
	author = {Bryan, Jenny},
	month = dec,
	year = {2017}
}


@article{nuzzo_scientific_2014,
	title = {Scientific method: statistical errors},
	volume = {506},
	issn = {1476-4687},
	shorttitle = {Scientific method},
	doi = {10.1038/506150a},
	language = {eng},
	number = {7487},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = feb,
	year = {2014},
	pmid = {24522584},
	keywords = {Data Interpretation, Statistical, Data Mining, Reproducibility of Results, Research Design},
	pages = {150--152}
}



@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology} {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {methodology, motivated reasoning, publication, disclosure},
	pages = {1359--1366}
}


@misc{harrell_introduction_2017,
	title = {Introduction},
	url = {https://www.fharrell.com/post/introduction/},
	abstract = {Statistics is a field that is a science unto itself and that benefits all other fields and everyday life. What is unique about statistics is its proven tools for decision making in the face of uncertainty, understanding sources of variation and bias, and most importantly, statistical thinking.},
	language = {en-us},
	urldate = {2021-02-06},
	journal = {Statistical Thinking},
	author = {Harrell, Frank},
	month = jan,
	year = {2017}
}

@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}


@article{dushoff_i_2019,
	title = {I can see clearly now: {Reinterpreting} statistical significance},
	volume = {10},
	copyright = {© 2019 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society},
	issn = {2041-210X},
	shorttitle = {I can see clearly now},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13159},
	doi = {10.1111/2041-210X.13159},
	abstract = {Null hypothesis significance testing (NHST) remains popular despite decades of concern about misuse and misinterpretation. There are many recent suggestions for mitigating problems arising from NHST, including calls for abandoning NHST in favour of Bayesian or information-theoretic approaches. We believe that NHST will continue to be widely used, and can be most usefully interpreted as a guide to whether a certain effect can be seen clearly in a particular context (e.g. whether we can clearly see that a correlation or between-group difference is positive or negative). We believe that much misinterpretation of NHST is due to language: significance testing has little to do with other meanings of the word ‘significance’. We therefore suggest that researchers describe the conclusions of null-hypothesis tests in terms of statistical ‘clarity’ rather than ‘significance’. We illustrate our point by rewriting common misinterpretations of the meaning of statistical tests found in the literature using the language of ‘clarity’. The meaning of statistical tests become easier to interpret and explain when viewed through the lens of ‘statistical clarity’. Our suggestion is mild, but practical: this simple semantic change could enhance clarity in statistical communication.},
	language = {en},
	number = {6},
	urldate = {2019-06-18},
	journal = {Methods in Ecology and Evolution},
	author = {Dushoff, Jonathan and Kain, Morgan P. and Bolker, Benjamin M.},
	year = {2019},
	keywords = {p-value, hypothesis testing, null hypothesis significance testing, statistical clarity, statistical philosophy, statistical significance},
	pages = {756--759}
}

@article{davidoff_standing_1999,
	title = {Standing {Statistics} {Right} {Side} {Up}},
	volume = {130},
	issn = {0003-4819},
	url = {https://www-acpjournals-org.libaccess.lib.mcmaster.ca/doi/10.7326/0003-4819-130-12-199906150-00022},
	doi = {10.7326/0003-4819-130-12-199906150-00022},
	number = {12},
	urldate = {2021-02-06},
	journal = {Annals of Internal Medicine},
	author = {Davidoff, Frank},
	month = jun,
	year = {1999},
	note = {Publisher: American College of Physicians},
	pages = {1019--1021},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/U464VGSA/0003-4819-130-12-199906150-00022.html:text/html}
}


@misc{pigliucci_reject_2004,
	title = {Reject that null hypothesis!},
	url = {https://web.archive.org/web/20040820160247fw_/http://life.bio.sunysb.edu/ee/pigliuccilab/handouts/reject_null_hypothesis.pdf},
	abstract = {A short guide to why standard statistical hypothesis testing is not very useful,  and to what alternatives are available.},
	author = {Pigliucci, Massimo},
	year = {2004}
}


@article{berger_could_2003,
	title = {Could {Fisher}, {Jeffreys} and {Neyman} {Have} {Agreed} on {Testing}?},
	volume = {18},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1056397485},
	doi = {10.1214/ss/1056397485},
	abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions. This article focuses on discussion of the conditional frequentist approach to testing, which is argued to provide the basis for a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the "strength of evidence" in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II error probabilities, but do so conditional on the strength of evidence in the data. The resulting conditional frequentist error probabilities equal the objective posterior probabilities of the hypotheses advocated by Jeffreys.},
	language = {en},
	number = {1},
	urldate = {2021-02-06},
	journal = {Statistical Science},
	author = {Berger, James O.},
	month = feb,
	year = {2003},
	mrnumber = {MR1997064},
	zmnumber = {1048.62006},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {conditional testing., p-values, posterior probabilities of hypotheses, Type I and Type II error probabilities},
	pages = {1--32}
}


@article{mccullagh_what_2002,
	title = {What is a statistical model?},
	volume = {30},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1035844977},
	doi = {10.1214/aos/1035844977},
	abstract = {This paper addresses two closely related questions, "What is a statistical model?" and "What is a parameter?" The notions that a model must "make sense," and that a parameter must "have a well-defined meaning" are deeply ingrained in applied statistical work, reasonably well understood at an instinctive level, but absent from most formal theories of modelling and inference. In this paper, these concepts are defined in algebraic terms, using morphisms, functors and natural transformations. It is argued that inference on the basis of a model is not possible unless the model admits a natural extension that includes the domain for which inference is required. For example, prediction requires that the domain include all future units, subjects or time points. Although it is usually not made explicit, every sensible statistical model admits such an extension. Examples are given to show why such an extension is necessary and why a formal theory is required. In the definition of a subparameter, it is shown that certain parameter functions are natural and others are not. Inference is meaningful only for natural parameters. This distinction has important consequences for the construction of prior distributions and also helps to resolve a controversy concerning the Box-Cox model.},
	language = {en},
	number = {5},
	urldate = {2021-02-06},
	journal = {Annals of Statistics},
	author = {McCullagh, Peter},
	month = oct,
	year = {2002},
	mrnumber = {MR1936320},
	zmnumber = {1039.62003},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Aggregation, agricultural field experiment, Bayes inference, Box-Cox model, category, causal inference, commutative diagram, conformal model, contingency table, embedding, exchangeability, extendability, extensive variable, fertility effect, functor, Gibbs model, harmonic model, intensive variable, interference, Kolmogorov consistency, lattice process, measure process, morphism, natural parameterization, natural subparameter, opposite category, quadratic exponential model, representation, spatial process, spline model, type III model},
	pages = {1225--1310}
}


@article{gelman_difference_2006,
	title = {The {Difference} {Between} “{Significant}” and “{Not} {Significant}” is not {Itself} {Statistically} {Significant}},
	volume = {60},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313006X152649},
	doi = {10.1198/000313006X152649},
	language = {en},
	number = {4},
	urldate = {2015-11-10},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Stern, Hal},
	month = nov,
	year = {2006},
	pages = {328--331},
	file = {signif4.pdf:/home/bolker/Documents/zotero_new/storage/I64MSHDR/signif4.pdf:application/pdf}
}


@article{nieuwenhuis_erroneous_2011,
	title = {Erroneous analyses of interactions in neuroscience: a problem of significance},
	volume = {14},
	copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	shorttitle = {Erroneous analyses of interactions in neuroscience},
	url = {https://www.nature.com/articles/nn.2886},
	doi = {10.1038/nn.2886},
	abstract = {The authors analyze a large corpus of the neuroscience literature and demonstrate that nearly half of the published studies considered incorrectly compared effect sizes by comparing their significance levels.},
	language = {en},
	number = {9},
	urldate = {2021-02-06},
	journal = {Nature Neuroscience},
	author = {Nieuwenhuis, Sander and Forstmann, Birte U. and Wagenmakers, Eric-Jan},
	month = sep,
	year = {2011},
	note = {Number: 9
Publisher: Nature Publishing Group},
	pages = {1105--1107},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/2YDPX3R9/Nieuwenhuis et al. - 2011 - Erroneous analyses of interactions in neuroscience.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/BQVH2CHL/nn.html:text/html}
}

@article{goldacre_statistical_2011,
	title = {The statistical error that just keeps on coming},
	url = {http://www.theguardian.com/commentisfree/2011/sep/09/bad-science-research-error},
	abstract = {The same statistical errors – namely, ignoring the "difference in differences" – are appearing throughout the most prestigious journals in neuroscience},
	language = {en},
	urldate = {2021-02-06},
	journal = {The Guardian},
	author = {Goldacre, Ben},
	month = sep,
	year = {2011},
	note = {Section: Opinion},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/FDSBGD6R/bad-science-research-error.html:text/html}
}

@article{gerber_publication_2008,
	title = {Publication {Bias} in {Empirical} {Sociological} {Research}: {Do} {Arbitrary} {Significance} {Levels} {Distort} {Published} {Results}?},
	volume = {37},
	issn = {0049-1241},
	shorttitle = {Publication {Bias} in {Empirical} {Sociological} {Research}},
	url = {https://doi.org/10.1177/0049124108318973},
	doi = {10.1177/0049124108318973},
	abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
	language = {en},
	number = {1},
	urldate = {2021-02-06},
	journal = {Sociological Methods \& Research},
	author = {Gerber, Alan S. and Malhotra, Neil},
	month = aug,
	year = {2008},
	note = {Publisher: SAGE Publications Inc},
	keywords = {caliper test, hypothesis testing, meta-analysis, publication bias},
	pages = {3--30},
	file = {SAGE PDF Full Text:/home/bolker/Documents/zotero_new/storage/SJPPR4MV/Gerber and Malhotra - 2008 - Publication Bias in Empirical Sociological Researc.pdf:application/pdf}
}


@article{franklin_millikans_1981,
	title = {Millikan's {Published} and {Unpublished} {Data} on {Oil} {Drops}},
	volume = {11},
	issn = {0073-2672},
	url = {https://www.jstor.org/stable/27757478},
	doi = {10.2307/27757478},
	number = {2},
	urldate = {2021-02-11},
	journal = {Historical Studies in the Physical Sciences},
	author = {Franklin, Allan D.},
	year = {1981},
	pages = {185--201}
}

@book{gotelli_primer_2004,
	address = {Sunderland, MA},
	title = {A {Primer} of {Ecological} {Statistics}},
	publisher = {Sinauer},
	author = {Gotelli, Nicholas J. and Ellison, Aaron M.},
	year = {2004}
}

@article{schielzeth_simple_2010,
	title = {Simple means to improve the interpretability of regression coefficients},
	url = {http://dx.doi.org/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	abstract = {1. Linear regression models are an important statistical tool in evolutionary and ecological studies. Unfortunately, these models often yield some uninterpretable estimates and hypothesis tests, especially when models contain interactions or polynomial terms. Furthermore, the standard errors for treatment groups, although often of interest for including in a publication, are not directly available in a standard linear model. 2. Centring and standardization of input variables are simple means to improve the interpretability of regression coefficients. Further, refitting the model with a slightly modified model structure allows extracting the appropriate standard errors for treatment groups directly from the model. 3. Centring will make main effects biologically interpretable even when involved in interactions and thus avoids the potential misinterpretation of main effects. This also applies to the estimation of linear effects in the presence of polynomials. Categorical input variables can also be centred and this sometimes assists interpretation. 4. Standardization (z-transformation) of input variables results in the estimation of standardized slopes or standardized partial regression coefficients. Standardized slopes are comparable in magnitude within models as well as between studies. They have some advantages over partial correlation coefficients and are often the more interesting standardized effect size. 5. The thoughtful removal of intercepts or main effects allows extracting treatment means or treatment slopes and their appropriate standard errors directly from a linear model. This provides a simple alternative to the more complicated calculation of standard errors from contrasts and main effects. 6. The simple methods presented here put the focus on parameter estimation (point estimates as well as confidence intervals) rather than on significance thresholds. They allow fitting complex, but meaningful models that can be concisely presented and interpreted. The presented methods can also be applied to generalised linear models {(GLM)} and linear mixed models.},
	volume = {1},
pages = {103-113},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	year = {2010}
}

@book{faraway_extending_2016,
title = {Extending {Linear} {Models} with {R}: {Generalized} {Linear}, {Mixed} {Effects} and {Nonparametric} {Regression} {Models}},
	publisher = {Chapman \& Hall/CRC},
	author = {Faraway, Julian J.},
	edition={2},
	year = {2016}
}

@article{warton_arcsine_2011,
	title = {The arcsine is asinine: the analysis of proportions in ecology},
	volume = {92},
	issn = {0012-9658},
	shorttitle = {The arcsine is asinine},
	url = {http://www.esajournals.org/doi/full/10.1890/10-0340.1},
	doi = {10.1890/10-0340.1},
	journal = {Ecology},
	author = {Warton, David I. and Hui, Francis K. C.},
	month = jan,
	year = {2011},
	pages = {3--10}
}


@article{morrissey_revisiting_2020,
	title = {Revisiting advice on the analysis of count data},
	volume = {11},
	number = {9},
	journal = {Methods in Ecology and Evolution},
	author = {Morrissey, Michael B. and Ruxton, Graeme D.},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {1133--1140},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/S4UH2KUJ/2041-210X.html:text/html}
}

@article{st-pierre_count_2018,
	title = {Count data in biology—{Data} transformation or model reformation?},
	volume = {8},
	number = {6},
	journal = {Ecology and evolution},
	author = {St-Pierre, Anne P. and Shikon, Violaine and Schneider, David C.},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {3077--3085},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/679LQLZC/ece3.html:text/html}
}

@article{ohara_not_2010,
	title = {Do not log-transform count data},
	journal = {Nature Precedings},
	author = {O'Hara, Robert and Kotze, Johan},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {1--1},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/CXV4LTPE/O'Hara and Kotze - 2010 - Do not log-transform count data.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/E63JXMXR/npre.2010.4136.html:text/html}
}

@article{ives_for_2015,
	title = {For testing the significance of regression coefficients, go ahead and log-transform count data},
	volume = {6},
	number = {7},
	journal = {Methods in Ecology and Evolution},
	author = {Ives, Anthony R.},
	year = {2015},
	note = {Publisher: Wiley Online Library},
	pages = {828--835},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/84DR7C3B/2041-210x.html:text/html}
}

@article{warton_three_2016,
	title = {Three points to consider when choosing a {LM} or {GLM} test for count data},
	volume = {7},
	number = {8},
	journal = {Methods in Ecology and Evolution},
	author = {Warton, David I. and Lyons, Mitchell and Stoklosa, Jakub and Ives, Anthony R.},
	year = {2016},
	note = {Publisher: Wiley Online Library},
	pages = {882--890}
}


@article{gelman_lets_2002,
	title = {Let's {Practice} {What} {We} {Preach}: {Turning} {Tables} into {Graphs}},
	volume = {56},
	issn = {0003-1305},
	shorttitle = {Let's {Practice} {What} {We} {Preach}},
	url = {http://www.jstor.org/stable/3087382},
	abstract = {{\textless}p{\textgreater}Statisticians recommend graphical displays but often use tables to present their own research results. Could graphs do better? We study the question by going through the tables in a recent issue of the Journal of the American Statistical Association. We show how it is possible to improve the presentations using graphs that actually take up less space than the original tables. We find a particularly effective tool to be multiple repeated line plots, with comparisons of interest connected by lines and separate comparisons isolated on different plots.},
	number = {2},
	urldate = {2011-06-29},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Pasarica, Cristian and Dodhia, Rahul},
	month = may,
	year = {2002},
	note = {ArticleType: research-article / Full publication date: May, 2002 / Copyright © 2002 American Statistical Association},
	pages = {121--130},
	file = {JSTOR Full Text PDF:/home/bolker/Documents/zotero_new/storage/VVIX5J5A/Gelman et al. - 2002 - Let's Practice What We Preach Turning Tables into.pdf:application/pdf}
}



@book{dobson_introduction_2008,
	edition = {3},
	title = {An {Introduction} to {Generalized} {Linear} {Models}},
	isbn = {1-58488-950-0},
	publisher = {Chapman and Hall/CRC},
	author = {Dobson, Annette J. and Barnett, Adrian},
	month = may,
	year = {2008}
}


@article{schielzeth_conclusions_2009,
	title = {Conclusions beyond support: overconfident estimates in mixed models},
	volume = {20},
	issn = {1045-2249, 1465-7279},
	shorttitle = {Conclusions beyond support},
	url = {http://beheco.oxfordjournals.org/content/20/2/416},
	doi = {10.1093/beheco/arn145},
	abstract = {Mixed-effect models are frequently used to control for the nonindependence of data points, for example, when repeated measures from the same individuals are available. The aim of these models is often to estimate fixed effects and to test their significance. This is usually done by including random intercepts, that is, intercepts that are allowed to vary between individuals. The widespread belief is that this controls for all types of pseudoreplication within individuals. Here we show that this is not the case, if the aim is to estimate effects that vary within individuals and individuals differ in their response to these effects. In these cases, random intercept models give overconfident estimates leading to conclusions that are not supported by the data. By allowing individuals to differ in the slopes of their responses, it is possible to account for the nonindependence of data points that pseudoreplicate slope information. Such random slope models give appropriate standard errors and are easily implemented in standard statistical software. Because random slope models are not always used where they are essential, we suspect that many published findings have too narrow confidence intervals and a substantially inflated type I error rate. Besides reducing type I errors, random slope models have the potential to reduce residual variance by accounting for between-individual variation in slopes, which makes it easier to detect treatment effects that are applied between individuals, hence reducing type II errors as well.},
	language = {en},
	number = {2},
	urldate = {2012-07-27},
	journal = {Behavioral Ecology},
	author = {Schielzeth, Holger and Forstmeier, Wolfgang},
	month = mar,
	year = {2009},
	keywords = {repeated measures, mixed-effect models, type I error, experimental design, maternal effects, random regression},
	pages = {416--420}
}

@article{barr_random_2013,
	title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
	volume = {68},
	issn = {0749-{596X}},
	shorttitle = {Random effects structure for confirmatory hypothesis testing},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X12001180},
	doi = {10.1016/j.jml.2012.11.001},
	abstract = {Linear mixed-effects models ({LMEMs)} have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using {LMEMs} for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that {LMEMs} generalize best when they include the maximal random effects structure justified by the design. The generalization performance of {LMEMs} including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only {LMEMs} used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal {LMEMs} should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
	number = {3},
	urldate = {2013-09-26},
	journal = {Journal of Memory and Language},
	author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
	month = apr,
	year = {2013},
	keywords = {Generalization, Linear mixed-effects models, Monte Carlo simulation, statistics},
	pages = {255--278}
}

@article{matuschek_balancing_2017,
journal={Journal of Memory and Language},
year=2017,
volume=94,
pages={305-315},
doi={10.1016/j.jml.2017.01.001},
	title = {Balancing Type {I} Error and Power in Linear Mixed Models},
	abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. The advantages of LMMs over ANOVAs, however, come at a cost: Setting up an LMM is not as straightforward as running an ANOVA. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr et al., 2013), presumably to keep Type I error down to the nominal \${\textbackslash}alpha\$ in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, models with a random effect structure that is supported by the data have optimal Type I error and power properties.},
	author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
}

@article{bates_parsimonious_2015,
	title = {Parsimonious {Mixed} {Models}},
	url = {http://arxiv.org/abs/1506.04967},
	abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr et al. (2013) recommended fitting 'maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification. Finally, we clarify that the simulations on which Barr et al. base their recommendations are atypical for real data. A detailed example is provided of how subject-related attentional fluctuation across trials may further qualify statistical inferences about fixed effects, and of how such nonlinear effects can be accommodated within the mixed-effects modeling framework.},
	urldate = {2015-12-31},
	journal = {arXiv:1506.04967 [stat]},
	author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04967},
	keywords = {Statistics - Methodology}
}


@article{dworkin_evidence_2005,
	title = {Evidence for canalization of {Distal}-less function in the leg of {{\em Drosophila melanogaster}}},
	volume = {7},
	issn = {1525-142X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1525-142X.2005.05010.x},
	doi = {10.1111/j.1525-142X.2005.05010.x},
	abstract = {A considerable body of theory pertaining to the evolution of canalization has emerged recently, yet there have been few empirical investigations of their predictions. To address this, patterns of canalization and trait correlation were investigated under the individual and joint effects of the introgression of a loss-of-function allele of the Distal-less gene and high-temperature stress on a panel of iso-female lines. Variation was examined for number of sex comb teeth and the length of the basi-tarsus on the pro-thoracic leg of male Drosophila melanogaster. I demonstrate that whereas there is evidence for trait canalization, there is no evidence to support the hypothesis of the evolution of genetic canalization as a response to microenvironmental canalization. Furthermore, I demonstrate that although there are genetic correlations between these traits, there is no association between their measures of canalization. I discuss the prospects of the evolutionary lability of the Distal-less gene within the context of changes in genetic variation and covariation.},
	language = {en},
	number = {2},
	urldate = {2021-06-12},
	journal = {Evolution \& Development},
	author = {Dworkin, Ian},
	year = {2005},
	pages = {89--100}
}

@article{wilson_good_2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2019-03-16},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code}
}


