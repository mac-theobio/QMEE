
@article{paczolt_multiple_2015,
	title = {Multiple Mating and Reproductive Skew in Parental and Introgressed Females of the Live-Bearing Fish {{\em Xiphophorus birchmanni}}},
	volume = {106},
	issn = {0022-1503},
	url = {https://academic.oup.com/jhered/article/106/1/57/2960035},
	doi = {10.1093/jhered/esu066},
	language = {en},
	number = {1},
	urldate = {2019-02-03},
	journal = {Journal of Heredity},
	author = {Paczolt, Kimberly A. and Passow, Courtney N. and Delclos, Pablo J. and Kindsvater, Holly K. and Jones, Adam G. and Rosenthal, Gil G.},
	month = jan,
	year = {2015},
	pages = {57--66}
}

@article{hurlbert_pseudoreplication_1984,
	title = {Pseudoreplication and the {Design} of {Ecological} {Field} {Experiments}},
	volume = {54},
	issn = {0012-9615},
	url = {http://www.esajournals.org/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre—layout (or conventional) and layout—specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.  See full-text article at JSTOR},
	number = {2},
	urldate = {2015-11-08},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = jun,
	year = {1984},
	pages = {187--211}
}

@article{davies_dont_2015,
	title = {Don't let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring)},
	copyright = {© 2015 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {2045-7758},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/ece3.1782/abstract},
	doi = {10.1002/ece3.1782},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects where treatments are not replicated and/or replicates are not statistically independent. It is a genuine but controversial issue in ecology particularly in the case of costly landscape-scale manipulations, behavioral studies where ethics or other concerns may limit sample sizes, ad hoc monitoring data, and the analysis of natural experiments where chance events occur at a single site. Here key publications on the topic are reviewed to illustrate the debate that exists about the conceptual validity of pseudoreplication. A survey of ecologists and case studies of experimental design and publication issues are used to explore the extent of the problem, ecologists’ solutions, reviewers’ attitudes, and the fate of submitted manuscripts. Scientists working across a range of ecological disciplines regularly come across the problem of pseudoreplication and build solutions into their designs and analyses. These include carefully defining hypotheses and the population of interest, acknowledging the limits of statistical inference and using statistical approaches including nesting and random effects. Many ecologists face considerable challenges getting their work published if accusations of pseudoreplication are made – even if the problem has been dealt with. Many reviewers reject papers for pseudoreplication, and this occurs more often if they haven't experienced the issue themselves. The concept of pseudoreplication is being applied too dogmatically and often leads to rejection during review. There is insufficient consideration of the associated philosophical issues and potential statistical solutions. By stopping the publication of ecological studies, reviewers are slowing the pace of ecological research and limiting the scope of management case studies, natural events studies, and valuable data available to form evidence-based solutions. Recommendations for fair and consistent treatment of pseudoreplication during writing and review are given for authors, reviewers, and editors.},
	language = {en},
	urldate = {2015-11-08},
	journal = {Ecology and Evolution},
	author = {Davies, G. Matt and Gray, Alan},
	month = oct,
	year = {2015},
	keywords = {Bayesian statistics, confounded effects, hypothesis formation, nesting, peer review, P-values, random effects, scientific publication, statistical population}
}

@article{gelman_beyond_2014,
	title = {Beyond Power Calculations: Assessing Type {S} (Sign) and Type {M} (Magnitude) Errors},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/content/9/6/641},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2015-11-08},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	pmid = {26186114},
	keywords = {design calculation, exaggeration ratio, power analysis, replication crisis, statistical significance, Type M error, Type S error},
	pages = {641--651}
}

@misc{lenth_java_2006,
	title = {Java {Applets} for {Power} and {Sample} {Size} [computer software]},
	url = {http://www.stat.uiowa.edu/~rlenth/Power},
	author = {Lenth, R. V.},
	year = {2006}
}


@article{faul_statistical_2009,
	title = {Statistical power analyses using {G}*{Power} 3.1: {Tests} for correlation and regression analyses},
	volume = {41},
	issn = {1554-3528},
	shorttitle = {Statistical power analyses using {G}*{Power} 3.1},
	url = {https://doi.org/10.3758/BRM.41.4.1149},
	doi = {10.3758/BRM.41.4.1149},
	abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
	language = {en},
	number = {4},
	urldate = {2019-03-17},
	journal = {Behavior Research Methods},
	author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
	month = nov,
	year = {2009},
	keywords = {Effect Size Measure, Implicit Association Test, Linear Multiple Regression, Multiple Correlation Coefficient, Noncentrality Parameter},
	pages = {1149--1160}
}


@article{gerard_limits_1998,
	title = {Limits of {Retrospective} {Power} {Analysis}},
	volume = {62},
	issn = {0022541X},
	url = {http://www.jstor.org/stable/3802357},
	abstract = {Power analysis after study completion has been suggested to interpret study results. We present 3 methods of estimating power and discuss their limitations. We use simulation studies to show that estimated power can be biased, extremely variable, and severely bounded. We endorse the practice of computing power to detect a biologically meaningful difference as a tool for study planning but suggest that calculation of confidence intervals on the parameter of interest is the appropriate way to gauge the strength and biological meaning of study results.},
	number = {2},
	urldate = {2010-10-25},
	journal = {The Journal of Wildlife Management},
	author = {Gerard, Patrick D. and Smith, David R. and Weerakkody, Govinda},
	month = apr,
	year = {1998},
	note = {ArticleType: research-article / Full publication date: Apr., 1998 / Copyright © 1998 Allen Press},
	pages = {801--807}
}

@article{thomas_retrospective_1997,
	title = {Retrospective {Power} {Analysis}},
	volume = {11},
	issn = {0888-8892},
	url = {http://onlinelibrary.wiley.com/doi/10.1046/j.1523-1739.1997.96102.x/abstract;jsessionid=6D27EBA7A0ED9B0498B38A0F03A0D521.d02t01},
	doi = {10.1046/j.1523-1739.1997.96102.x},
	number = {1},
	urldate = {2010-10-25},
	journal = {Conservation Biology},
	author = {Thomas, Len},
	month = feb,
	year = {1997},
	pages = {276--280}
}


@book{bolker_ecological_2008,
	title = {Ecological {Models} and {Data} in {R}},
	isbn = {0-691-12522-8},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	month = jul,
	year = {2008}
}


@article{hoenig_abuse_2001,
	title = {The {Abuse} of {Power}},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001300339897},
	doi = {10.1198/000313001300339897},
	abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
	number = {1},
	urldate = {2019-03-18},
	journal = {The American Statistician},
	author = {Hoenig, John M. and Heisey, Dennis M.},
	month = feb,
	year = {2001},
	keywords = {Bioequivalence testing, Burden of proof, Observed power, Retrospective power analysis, Statistical power, Type II error},
	pages = {19--24}
}

@article{baath_state_2012,
	title = {The {State} of {Naming} {Conventions} in {R}},
	volume = {4},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2012/RJ-2012-018/index.html},
	language = {en},
	number = {2},
	urldate = {2021-01-14},
	journal = {The R Journal},
	author = {Bååth, Rasmus},
	year = {2012},
	pages = {74--75}
}


@misc{bryan_project-oriented_2017,
	title = {Project-oriented workflow},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
	language = {en-us},
	urldate = {2021-01-14},
	journal = {Tidyverse},
	author = {Bryan, Jenny},
	month = dec,
	year = {2017}
}


@article{nuzzo_scientific_2014,
	title = {Scientific method: statistical errors},
	volume = {506},
	issn = {1476-4687},
	shorttitle = {Scientific method},
	doi = {10.1038/506150a},
	language = {eng},
	number = {7487},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = feb,
	year = {2014},
	pmid = {24522584},
	keywords = {Data Interpretation, Statistical, Data Mining, Reproducibility of Results, Research Design},
	pages = {150--152}
}



@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology} {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {methodology, motivated reasoning, publication, disclosure},
	pages = {1359--1366}
}


@misc{harrell_introduction_2017,
	title = {Introduction},
	url = {https://www.fharrell.com/post/introduction/},
	abstract = {Statistics is a field that is a science unto itself and that benefits all other fields and everyday life. What is unique about statistics is its proven tools for decision making in the face of uncertainty, understanding sources of variation and bias, and most importantly, statistical thinking.},
	language = {en-us},
	urldate = {2021-02-06},
	journal = {Statistical Thinking},
	author = {Harrell, Frank},
	month = jan,
	year = {2017}
}

@article{gelman_statistical_2014,
	title = {The statistical crisis in science: data-dependent analysis--a "garden of forking paths"--explains why many statistically significant comparisons don't hold up},
	volume = {102},
	issn = {0003-0996},
	shorttitle = {The statistical crisis in science},
	url = {http://link.galegroup.com/apps/doc/A389260653/AONE?u=ocul_mcmaster&sid=AONE&xid=4f4562c0},
	language = {English},
	number = {6},
	urldate = {2019-01-07},
	journal = {American Scientist},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	note = {460},
	keywords = {Periodical publishing, Science journals},
	pages = {460--}
}


@article{dushoff_i_2019,
	title = {I can see clearly now: {Reinterpreting} statistical significance},
	volume = {10},
	copyright = {© 2019 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society},
	issn = {2041-210X},
	shorttitle = {I can see clearly now},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13159},
	doi = {10.1111/2041-210X.13159},
	abstract = {Null hypothesis significance testing (NHST) remains popular despite decades of concern about misuse and misinterpretation. There are many recent suggestions for mitigating problems arising from NHST, including calls for abandoning NHST in favour of Bayesian or information-theoretic approaches. We believe that NHST will continue to be widely used, and can be most usefully interpreted as a guide to whether a certain effect can be seen clearly in a particular context (e.g. whether we can clearly see that a correlation or between-group difference is positive or negative). We believe that much misinterpretation of NHST is due to language: significance testing has little to do with other meanings of the word ‘significance’. We therefore suggest that researchers describe the conclusions of null-hypothesis tests in terms of statistical ‘clarity’ rather than ‘significance’. We illustrate our point by rewriting common misinterpretations of the meaning of statistical tests found in the literature using the language of ‘clarity’. The meaning of statistical tests become easier to interpret and explain when viewed through the lens of ‘statistical clarity’. Our suggestion is mild, but practical: this simple semantic change could enhance clarity in statistical communication.},
	language = {en},
	number = {6},
	urldate = {2019-06-18},
	journal = {Methods in Ecology and Evolution},
	author = {Dushoff, Jonathan and Kain, Morgan P. and Bolker, Benjamin M.},
	year = {2019},
	keywords = {p-value, hypothesis testing, null hypothesis significance testing, statistical clarity, statistical philosophy, statistical significance},
	pages = {756--759}
}

@article{davidoff_standing_1999,
	title = {Standing {Statistics} {Right} {Side} {Up}},
	volume = {130},
	issn = {0003-4819},
	url = {https://www-acpjournals-org.libaccess.lib.mcmaster.ca/doi/10.7326/0003-4819-130-12-199906150-00022},
	doi = {10.7326/0003-4819-130-12-199906150-00022},
	number = {12},
	urldate = {2021-02-06},
	journal = {Annals of Internal Medicine},
	author = {Davidoff, Frank},
	month = jun,
	year = {1999},
	note = {Publisher: American College of Physicians},
	pages = {1019--1021},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/U464VGSA/0003-4819-130-12-199906150-00022.html:text/html}
}


@misc{pigliucci_reject_2004,
	title = {Reject that null hypothesis!},
	url = {https://web.archive.org/web/20040820160247fw_/http://life.bio.sunysb.edu/ee/pigliuccilab/handouts/reject_null_hypothesis.pdf},
	abstract = {A short guide to why standard statistical hypothesis testing is not very useful,  and to what alternatives are available.},
	author = {Pigliucci, Massimo},
	year = {2004}
}


@article{berger_could_2003,
	title = {Could {Fisher}, {Jeffreys} and {Neyman} {Have} {Agreed} on {Testing}?},
	volume = {18},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1056397485},
	doi = {10.1214/ss/1056397485},
	abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions. This article focuses on discussion of the conditional frequentist approach to testing, which is argued to provide the basis for a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the "strength of evidence" in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II error probabilities, but do so conditional on the strength of evidence in the data. The resulting conditional frequentist error probabilities equal the objective posterior probabilities of the hypotheses advocated by Jeffreys.},
	language = {en},
	number = {1},
	urldate = {2021-02-06},
	journal = {Statistical Science},
	author = {Berger, James O.},
	month = feb,
	year = {2003},
	mrnumber = {MR1997064},
	zmnumber = {1048.62006},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {conditional testing., p-values, posterior probabilities of hypotheses, Type I and Type II error probabilities},
	pages = {1--32}
}


@article{mccullagh_what_2002,
	title = {What is a statistical model?},
	volume = {30},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1035844977},
	doi = {10.1214/aos/1035844977},
	abstract = {This paper addresses two closely related questions, "What is a statistical model?" and "What is a parameter?" The notions that a model must "make sense," and that a parameter must "have a well-defined meaning" are deeply ingrained in applied statistical work, reasonably well understood at an instinctive level, but absent from most formal theories of modelling and inference. In this paper, these concepts are defined in algebraic terms, using morphisms, functors and natural transformations. It is argued that inference on the basis of a model is not possible unless the model admits a natural extension that includes the domain for which inference is required. For example, prediction requires that the domain include all future units, subjects or time points. Although it is usually not made explicit, every sensible statistical model admits such an extension. Examples are given to show why such an extension is necessary and why a formal theory is required. In the definition of a subparameter, it is shown that certain parameter functions are natural and others are not. Inference is meaningful only for natural parameters. This distinction has important consequences for the construction of prior distributions and also helps to resolve a controversy concerning the Box-Cox model.},
	language = {en},
	number = {5},
	urldate = {2021-02-06},
	journal = {Annals of Statistics},
	author = {McCullagh, Peter},
	month = oct,
	year = {2002},
	mrnumber = {MR1936320},
	zmnumber = {1039.62003},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Aggregation, agricultural field experiment, Bayes inference, Box-Cox model, category, causal inference, commutative diagram, conformal model, contingency table, embedding, exchangeability, extendability, extensive variable, fertility effect, functor, Gibbs model, harmonic model, intensive variable, interference, Kolmogorov consistency, lattice process, measure process, morphism, natural parameterization, natural subparameter, opposite category, quadratic exponential model, representation, spatial process, spline model, type III model},
	pages = {1225--1310}
}


@article{gelman_difference_2006,
	title = {The {Difference} {Between} “{Significant}” and “{Not} {Significant}” is not {Itself} {Statistically} {Significant}},
	volume = {60},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313006X152649},
	doi = {10.1198/000313006X152649},
	language = {en},
	number = {4},
	urldate = {2015-11-10},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Stern, Hal},
	month = nov,
	year = {2006},
	pages = {328--331},
	file = {signif4.pdf:/home/bolker/Documents/zotero_new/storage/I64MSHDR/signif4.pdf:application/pdf}
}


@article{nieuwenhuis_erroneous_2011,
	title = {Erroneous analyses of interactions in neuroscience: a problem of significance},
	volume = {14},
	copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	shorttitle = {Erroneous analyses of interactions in neuroscience},
	url = {https://www.nature.com/articles/nn.2886},
	doi = {10.1038/nn.2886},
	abstract = {The authors analyze a large corpus of the neuroscience literature and demonstrate that nearly half of the published studies considered incorrectly compared effect sizes by comparing their significance levels.},
	language = {en},
	number = {9},
	urldate = {2021-02-06},
	journal = {Nature Neuroscience},
	author = {Nieuwenhuis, Sander and Forstmann, Birte U. and Wagenmakers, Eric-Jan},
	month = sep,
	year = {2011},
	note = {Number: 9
Publisher: Nature Publishing Group},
	pages = {1105--1107},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/2YDPX3R9/Nieuwenhuis et al. - 2011 - Erroneous analyses of interactions in neuroscience.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/BQVH2CHL/nn.html:text/html}
}

@article{goldacre_statistical_2011,
	title = {The statistical error that just keeps on coming},
	url = {http://www.theguardian.com/commentisfree/2011/sep/09/bad-science-research-error},
	abstract = {The same statistical errors – namely, ignoring the "difference in differences" – are appearing throughout the most prestigious journals in neuroscience},
	language = {en},
	urldate = {2021-02-06},
	journal = {The Guardian},
	author = {Goldacre, Ben},
	month = sep,
	year = {2011},
	note = {Section: Opinion},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/FDSBGD6R/bad-science-research-error.html:text/html}
}

@article{gerber_publication_2008,
	title = {Publication {Bias} in {Empirical} {Sociological} {Research}: {Do} {Arbitrary} {Significance} {Levels} {Distort} {Published} {Results}?},
	volume = {37},
	issn = {0049-1241},
	shorttitle = {Publication {Bias} in {Empirical} {Sociological} {Research}},
	url = {https://doi.org/10.1177/0049124108318973},
	doi = {10.1177/0049124108318973},
	abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
	language = {en},
	number = {1},
	urldate = {2021-02-06},
	journal = {Sociological Methods \& Research},
	author = {Gerber, Alan S. and Malhotra, Neil},
	month = aug,
	year = {2008},
	note = {Publisher: SAGE Publications Inc},
	keywords = {caliper test, hypothesis testing, meta-analysis, publication bias},
	pages = {3--30},
	file = {SAGE PDF Full Text:/home/bolker/Documents/zotero_new/storage/SJPPR4MV/Gerber and Malhotra - 2008 - Publication Bias in Empirical Sociological Researc.pdf:application/pdf}
}


@article{franklin_millikans_1981,
	title = {Millikan's {Published} and {Unpublished} {Data} on {Oil} {Drops}},
	volume = {11},
	issn = {0073-2672},
	url = {https://www.jstor.org/stable/27757478},
	doi = {10.2307/27757478},
	number = {2},
	urldate = {2021-02-11},
	journal = {Historical Studies in the Physical Sciences},
	author = {Franklin, Allan D.},
	year = {1981},
	pages = {185--201}
}

@book{gotelli_primer_2004,
	address = {Sunderland, MA},
	title = {A {Primer} of {Ecological} {Statistics}},
	publisher = {Sinauer},
	author = {Gotelli, Nicholas J. and Ellison, Aaron M.},
	year = {2004}
}

@article{schielzeth_simple_2010,
	title = {Simple means to improve the interpretability of regression coefficients},
	url = {http://dx.doi.org/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	abstract = {1. Linear regression models are an important statistical tool in evolutionary and ecological studies. Unfortunately, these models often yield some uninterpretable estimates and hypothesis tests, especially when models contain interactions or polynomial terms. Furthermore, the standard errors for treatment groups, although often of interest for including in a publication, are not directly available in a standard linear model. 2. Centring and standardization of input variables are simple means to improve the interpretability of regression coefficients. Further, refitting the model with a slightly modified model structure allows extracting the appropriate standard errors for treatment groups directly from the model. 3. Centring will make main effects biologically interpretable even when involved in interactions and thus avoids the potential misinterpretation of main effects. This also applies to the estimation of linear effects in the presence of polynomials. Categorical input variables can also be centred and this sometimes assists interpretation. 4. Standardization (z-transformation) of input variables results in the estimation of standardized slopes or standardized partial regression coefficients. Standardized slopes are comparable in magnitude within models as well as between studies. They have some advantages over partial correlation coefficients and are often the more interesting standardized effect size. 5. The thoughtful removal of intercepts or main effects allows extracting treatment means or treatment slopes and their appropriate standard errors directly from a linear model. This provides a simple alternative to the more complicated calculation of standard errors from contrasts and main effects. 6. The simple methods presented here put the focus on parameter estimation (point estimates as well as confidence intervals) rather than on significance thresholds. They allow fitting complex, but meaningful models that can be concisely presented and interpreted. The presented methods can also be applied to generalised linear models {(GLM)} and linear mixed models.},
	volume = {1},
pages = {103-113},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	year = {2010}
}

@book{faraway_extending_2016,
title = {Extending {Linear} {Models} with {R}: {Generalized} {Linear}, {Mixed} {Effects} and {Nonparametric} {Regression} {Models}},
	publisher = {Chapman \& Hall/CRC},
	author = {Faraway, Julian J.},
	edition={2},
	year = {2016}
}

@article{warton_arcsine_2011,
	title = {The arcsine is asinine: the analysis of proportions in ecology},
	volume = {92},
	issn = {0012-9658},
	shorttitle = {The arcsine is asinine},
	url = {http://www.esajournals.org/doi/full/10.1890/10-0340.1},
	doi = {10.1890/10-0340.1},
	journal = {Ecology},
	author = {Warton, David I. and Hui, Francis K. C.},
	month = jan,
	year = {2011},
	pages = {3--10}
}


@article{morrissey_revisiting_2020,
	title = {Revisiting advice on the analysis of count data},
	volume = {11},
	number = {9},
	journal = {Methods in Ecology and Evolution},
	author = {Morrissey, Michael B. and Ruxton, Graeme D.},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {1133--1140},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/S4UH2KUJ/2041-210X.html:text/html}
}

@article{st-pierre_count_2018,
	title = {Count data in biology—{Data} transformation or model reformation?},
	volume = {8},
	number = {6},
	journal = {Ecology and evolution},
	author = {St-Pierre, Anne P. and Shikon, Violaine and Schneider, David C.},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	pages = {3077--3085},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/679LQLZC/ece3.html:text/html}
}

@article{ohara_not_2010,
	title = {Do not log-transform count data},
	journal = {Nature Precedings},
	author = {O'Hara, Robert and Kotze, Johan},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {1--1},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/CXV4LTPE/O'Hara and Kotze - 2010 - Do not log-transform count data.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/E63JXMXR/npre.2010.4136.html:text/html}
}

@article{ives_for_2015,
	title = {For testing the significance of regression coefficients, go ahead and log-transform count data},
	volume = {6},
	number = {7},
	journal = {Methods in Ecology and Evolution},
	author = {Ives, Anthony R.},
	year = {2015},
	note = {Publisher: Wiley Online Library},
	pages = {828--835},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/84DR7C3B/2041-210x.html:text/html}
}

@article{warton_three_2016,
	title = {Three points to consider when choosing a {LM} or {GLM} test for count data},
	volume = {7},
	number = {8},
	journal = {Methods in Ecology and Evolution},
	author = {Warton, David I. and Lyons, Mitchell and Stoklosa, Jakub and Ives, Anthony R.},
	year = {2016},
	note = {Publisher: Wiley Online Library},
	pages = {882--890}
}
