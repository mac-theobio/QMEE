---
title: Linear models lecture
author: Jonathan Dushoff and Ben Bolker
---

# Introduction
 
## History
 
- ANOVA, ANCOVA, regression, $t$ test are all variations of the same animal, the *general* linear model
- Many people (including the R project) call it a linear model (`lm`)
to distinguish it from the *generalized* linear model (`glm`)
- Unfortunately SAS calls it `PROC GLM`

## (part of) the statistical universe

![](models-glmm.png)

## Extended linear models
 
- *Generalized* linear models can incorporate:
    - (Some) non-linear relationships
    - Non-normal response *families* (binomial, Poisson, ...)
 - *Mixed* models incorporate *random effects*
    - Categories in data represent samples from a population
    - e.g. species, sites, genes ...
    - Traditionally used to account for experimental blocks
 
# Basic theory

## Assumptions
 
- *Response variables* are linear functions of *input variables*, in turn based on *predictor variables*
     - Can have one or more input variables per predictor variable
     - Each input variable is associated with an estimated parameter (more about this later)
- *Errors* or *residuals* are Normally distributed
     - In other words, the difference between our model *predictions*
       and our observations is Normal
     - *not* assuming the marginal distribution is Normal
- Independence
- (Predictor variables measured without error)
 
## Machinery
 
- Leads naturally to a *least squares* fit - we get parameters that minimize the squared differences between predictions and observations
- Least squares fits have a lot of nice properties, including partitioning, and finding the middle
- Solution is a simple (!) matrix equation
- Sensitive to some departures from the assumptions - anomalous events tend to have a larger effect than they should
- Alternatives
 
## One-parameter variables
 
- Continuous predictor variable: estimate a straight line with one parameter 
     - Also implies one *input variable* -- we're not going to
		talk much about those
- $Y = a+bX$: $Y$ is the response, $X$ is the input variable   
($b$ is the *slope* - expected change in $Y$ per unit change in $X$)
- Categorical predictor variable with two categories: only one parameter
     - difference in predicted value between levels, or
    	code the categories as 0, 1 ... (*dummy variables*)
- Parameters are (usually) easy to interpret
- Think in terms of *confidence intervals* for the parameter
 
## Multi-parameter variables
 
- With more than two categories, there is more than one
input variable (parameter) associated with a single predictor variable
    - Why can't we just code them, for example as 0, 1, 2?
- Non-linear response to a predictor variable
    - Might be able to use a linear model! $Y = a + bX + cX^2$ is linear
in $a$ and $b$ (the unknowns)
    - **Don't** use simple polynomials: use either *orthogonal polynomials* (`poly` in R) or splines (`ns` in R, `mgcv` package)

## Interpreting multi-parameter variables is hard
 
- We can (and should) get a $p$ value for the variable as a whole
- But we can only get CIs on the parameters --- and there are
different ways to parameterize (*contrasts*)
- Think clearly about the *scientific* questions you have for this variable
- If you're just trying to control for it, just put it in and then ignore it!
- If you do have a clear scientific question, you should be able to
construct *contrasts* in such a way that you can test it.
- If you must, do pairwise comparisons, test each pair of variables for differences and make an `aabbc` list
 
## Interactions
 
- Interactions allow the value of one predictor to affect the
relationship between another predictor and the response variable
- Interpreting *main effects* in the presence
of interactions is tricky (*principle of marginality*)
- Your estimate of the effect of variable $B$ is no longer constant
- You need to pick a fixed point, or average in some way
- Example: $Y = a + b_1 X_1 + b_2 X_2 + b_{12} X_1*X_2$
- The response to $X_1$ is $Y = (a+b_2 X_2) + (b_1+b_{12}X_2) X_1$  
the response to $X_1$ *depends on* the value of $X_2$.
 
## An experimental example
 
- You want to know whether a drug treatment changes the
	metabolism of some rabbits
- You're using adult, prime-aged rabbits and keeping them under
	controlled conditions, so you don't expect their metabolism to
	change *without* the drug.
    - Not good enough!
- You also introduce some control rabbits and treat them exactly the
same, including giving them fake pills. You find no significant
change in the metabolism of the control rabbits through time
    - Still not good enough! Why?
 
## Testing interactions
 
- Use an *interaction*:
$$
M = a + B_x X + B_t t + B_{xt} Xt
$$
- The interaction term $B_{xt}$ represents the *difference in the response* between the two groups.
- It asks: **did the treatment group change differently than the control group**?
 
## Interactions and parameters
 
- In simple cases, the whole interaction term may use only
one parameter
- We can use CIs, and coefficient plots, and get a pretty good idea what's
going on
- In more complicated cases, interaction terms may have many parameters 
- These have all the interpretation problems of other multi-parameter
variables, or more
- Think about "differences in differences"

## Interactions: example

- Bear road-crossing
- Predictor variables: sex (categorical), road type (categorical: major/minor), road length (continuous)
- Two-way interactions
     - sex $\times$ road length: "are females more sensitive to amount of road than males?"
	 - sex $\times$ road type: "do females vary behaviour between road type more than males?"
	 - road type $\times$ road length: "does amount of road affect crossings differently for different road types?"

## Statistical philosophy
 
 - Don't accept the null hypothesis
    - Don't throw out predictors you wanted to test because
		they're not significant
	- Don't throw out interactions you wanted to test because
		they're not significant
- This may make your life harder, but it's worth it for the karma
     - 	There are techniques to deal with multiple predictors (e.g.,
		lasso regression)
     - 	There are ways to estimate main effects in the presence of
		interactions ("sum-to-zero contrasts" or "orthogonal
		interactions")
 
## Diagnostics
 
- Because the linear model is sensitive (sometimes!) to
assumptions, it is good to evaluate them
- Concerns:
    - *Heteroscedasticity* (does variance change across the
		data set, e.g. increasing variance with increasing mean?)
    - Linearity (does your model fit well?)
	- Normality (assuming no overall problems, do your
		**residuals** look Normal?)
    - Independence is hard to test
- Normality is the **least important** of these assumptions

## Default plots in R

Example needed
 
<!--- 
http://lalashan.mcmaster.ca/theobio/bio_708_2013/index.php/Evan_Borman/diagnostics
-->
 
## Transformations
 
- One way to deal with problems in model assumptions is by
	transforming one or more of your variables
- Transformations are not cheating: a transformed scale may be as
natural (or more natural) a way to think about your data as your
original scale
- The linear scale (no transformation) often has direct meaning, if
you are adding things up or scaling them (as in our ant example)
- The log scale is often the best scale for thinking about physical
quantities: 1:10 as 10:?
- The *log odds*, or *logit*, scale is often the best scale for
thinking about probabilities: 1%:10% as 10%:?
 
## Transformation tradeoffs
 
- A transformation may help you meet model assumptions
    - Homoscedasticity
    - Linearity
    - Normality
- But there is no guarantee that you can fix them all
- Piles of zeros are hard too (consider GLMs)
 
## Transformations to consider
 
- log-lin, lin-log and log-log for various sorts of exponential and
power relationships
- Box-Cox and Yeo-Johnson (see example page)
- Avoid classical 'transform then linear model' recommendations for
     - probability data (logistic, arcsin or arcsin-square root)
		or count data (log, log(1+x) or square root)
	 - Generally better to respect the structure of these data with a GLM
 
## Deciding whether to transform
 
- It's **not OK** to pick transformations based on trying different
ones and looking at P values
- It's probably OK to decide based on a measure of Normality of
residuals, however
    - *Box-Cox transformation* tries out transformations of the form
	$(y^\lambda-1)/\lambda$ ($\lambda=0$ corresponds to
	log-transformation)
    - *additivity and variance stabilizing transformations* (Tibshirani)
	are a fancier way to transform
 
# Tools for fitting and inference

## Basic tools

- `lm` fits a linear model
- `summary` prints statistics associated with the *parameters* that were fitted
- `arm::coefplot` and `dotwhisker` package are useful for *visualizing* these
- `anova` and `drop1` will attach P values to
variables that have more than one parameter (even groups of
variables, in the case of anova). `car::Anova` can also
be useful
- **BEWARE**: the standard ANOVA $P$ values are *sequential*
(early variables *do not* control for the presence of later
variables.  This is probably not what you want. (See `car::Anova`)

## Multiple comparisons
 
- One standard of practice is to take a variable-level P value and
then evaluate patterns in the response to significant variables
- Straightforward, but maybe not conservative
- `TukeyHSD` does multiple comparison tests on objects
produced by `aov`.  Use `glht` in the `multcomp` package
more generally.
- Note: `aov` is just another way of calling `lm`, whereas
`anova` *compares* different model fits.  Sorry for the
confusion.
 
## Plotting

- `plot` can be applied to an `lm` object to give you a nice set of diagnostic tests.
- `predict` can give predicted values, and standard errors.
- `simulate` simulates values from the fitted model
- Try `methods(class="lm")` to see all the possibilities ...
- In `ggplot`, `geom_smooth(method="lm")` fits a linear model
	to each group of data (i.e.  each group that you have identified by
	plotting it in a different colour, within a different facet, etc.

